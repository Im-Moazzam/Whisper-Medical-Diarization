{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k77rm9OlXcy"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git -q -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp -q -U"
      ],
      "metadata": {
        "id": "Oiyech3llb5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yt-dlp https://youtu.be/S4wWClQhZaA --format mp4 -o \"/content/%(id)s.%(ext)s\" -q\n",
        "!mv /content/S4wWClQhZaA.mp4 /content/audio.mp4"
      ],
      "metadata": {
        "id": "SJnq4u6Rlel-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/audio.mp4\" --model small --language English"
      ],
      "metadata": {
        "id": "n0xmc-yplf3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai-whisper resemblyzer pydub numpy scikit-learn"
      ],
      "metadata": {
        "id": "QGO5lT_Tlhum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from resemblyzer import VoiceEncoder, preprocess_wav\n",
        "from sklearn.cluster import DBSCAN\n",
        "from pydub import AudioSegment\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load Whisper & Resemblyzer\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "encoder = VoiceEncoder()\n",
        "\n",
        "# Transcribe\n",
        "audio_path = \"/content/audio.mp4\"\n",
        "result = whisper_model.transcribe(audio_path, word_timestamps=True)\n",
        "\n",
        "# Load audio for chunking\n",
        "audio = AudioSegment.from_file(audio_path, format=\"mp4\")\n",
        "\n",
        "# Break into chunks using Whisper segments\n",
        "segments = []\n",
        "for seg in result[\"segments\"]:\n",
        "    start_ms = int(seg[\"start\"] * 1000)\n",
        "    end_ms = int(seg[\"end\"] * 1000)\n",
        "    audio_chunk = audio[start_ms:end_ms]\n",
        "    chunk_path = f\"chunk_{seg['id']}.wav\"\n",
        "    audio_chunk.export(chunk_path, format=\"wav\")\n",
        "    segments.append((chunk_path, seg[\"start\"], seg[\"end\"], seg[\"text\"]))\n",
        "\n",
        "# Get speaker embeddings\n",
        "embeddings = []\n",
        "for chunk_path, _, _, _ in segments:\n",
        "    wav = preprocess_wav(chunk_path)\n",
        "    embedding = encoder.embed_utterance(wav)\n",
        "    embeddings.append(embedding)\n",
        "    os.remove(chunk_path)  # cleanup\n",
        "\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# DBSCAN clustering (auto speaker detection)\n",
        "clustering = DBSCAN(eps=0.55, min_samples=2).fit(embeddings)\n",
        "labels = clustering.labels_\n",
        "\n",
        "# Handle \"Unknown\" labels (-1) by assigning them to closest valid speaker\n",
        "for i, label in enumerate(labels):\n",
        "    if label == -1:\n",
        "        distances = [\n",
        "            (j, cosine(embeddings[i], embeddings[j]))\n",
        "            for j in range(len(labels)) if labels[j] != -1\n",
        "        ]\n",
        "        if distances:\n",
        "            closest_idx, _ = min(distances, key=lambda x: x[1])\n",
        "            labels[i] = labels[closest_idx]  # Reassign to nearest speaker\n",
        "\n",
        "# Show number of unique speakers\n",
        "num_speakers = len(set(labels))\n",
        "print(f\"\\nüîä Estimated number of speakers (after reassignment): {num_speakers}\\n\")\n",
        "\n",
        "# Output transcript with speaker labels\n",
        "for (_, start, end, text), label in zip(segments, labels):\n",
        "    speaker = f\"Speaker {label}\"\n",
        "    print(f\"[{speaker}] {start:.2f} - {end:.2f}: {text}\")\n"
      ],
      "metadata": {
        "id": "UKtJFtb9lhsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count lines per speaker\n",
        "speaker_counts = Counter(label for _, label in zip(segments, labels))\n",
        "\n",
        "# Total lines\n",
        "total_lines = len(segments)\n",
        "\n",
        "print(\"\\nSpeaker Line Counts:\")\n",
        "for label, count in speaker_counts.items():\n",
        "    speaker = f\"Speaker {label}\" if label != -1 else \"Unknown\"\n",
        "    print(f\"{speaker}: {count} lines\")\n",
        "\n",
        "print(f\"\\nTotal lines: {total_lines}\")"
      ],
      "metadata": {
        "id": "EGn7zaS0lhpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "LoRbqEnhlhnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_transcript = \"\"\n",
        "for (_, start, end, text), label in zip(segments, labels):\n",
        "    speaker = f\"Speaker {label}\"\n",
        "\n",
        "    formatted_transcript += f\"[{speaker}] {text}\\n\"\n",
        "\n",
        "print(formatted_transcript)"
      ],
      "metadata": {
        "id": "SyDMYn5Rlhk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are a medical professional creating a SOAP note from a patient conversation transcript.\n",
        "Generate a SOAP note based on the following transcript.\n",
        "\n",
        "{formatted_transcript}\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "soap_note = response.text\n",
        "print(soap_note)"
      ],
      "metadata": {
        "id": "rxeotopjlhiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def count_tokens(text, model_name=\"gpt-4\"):\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "input_tokens = count_tokens(prompt)\n",
        "print(f\"Input Tokens: {input_tokens}\")\n",
        "\n",
        "output_tokens = count_tokens(soap_note)\n",
        "print(f\"Output Tokens: {output_tokens}\")"
      ],
      "metadata": {
        "id": "ZAzR1YhGlhZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_instruction = \"\"\"\n",
        "You are a clinical intake assistant. Your task is to conduct a focused medical interview with a patient through short, structured chat messages.\n",
        "\n",
        "You must collect the following information in a clear, concise manner:\n",
        "1. What symptoms the patient is currently experiencing\n",
        "2. When the issue started\n",
        "3. What makes the symptoms better or worse\n",
        "4. How the symptoms feel (e.g., severity, quality, location)\n",
        "5. Any associated symptoms (e.g., fever, nausea, vision changes)\n",
        "6. Any past history of similar problems or known medical conditions\n",
        "\n",
        "Important instructions:\n",
        "- Do NOT ask for name, age, gender, lifestyle, or occupation.\n",
        "- Avoid excessive friendliness, emojis, or small talk.\n",
        "- Be direct and medical in tone, like a triage nurse or clinical assistant.\n",
        "- Ask only one question at a time and wait for the user‚Äôs response.\n",
        "- Summarize and show the collected data only when the user types \"done\".\n",
        "- Format the summary in structured sections with labels: ‚ÄúSymptoms‚Äù, ‚ÄúOnset‚Äù, ‚ÄúTriggers‚Äù, ‚ÄúPain Description‚Äù, ‚ÄúAssociated Symptoms‚Äù, ‚ÄúMedical History‚Äù.\n",
        "\n",
        "Begin by asking:\n",
        "\"Hey I'm Hami! How are you feeling today?\"\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "OM8D3Gwnlr1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    system_instruction=system_instruction\n",
        ")\n",
        "\n",
        "chat = model.start_chat(history=[])\n"
      ],
      "metadata": {
        "id": "eLxN4AkilrzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure genai with the API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    system_instruction=\"\"\"\n",
        "You are a clinical intake assistant. Your task is to conduct a focused medical interview with a patient through short, structured chat messages.\n",
        "\n",
        "You must collect the following information in a clear, concise manner:\n",
        "1. What symptoms the patient is currently experiencing\n",
        "2. When the issue started\n",
        "3. What makes the symptoms better or worse\n",
        "4. How the symptoms feel (e.g., severity, quality, location)\n",
        "5. Any associated symptoms (e.g., fever, nausea, vision changes)\n",
        "6. Any past history of similar problems or known medical conditions\n",
        "\n",
        "Important instructions:\n",
        "- Do NOT ask for name, age, gender, lifestyle, or occupation.\n",
        "- Avoid excessive friendliness, emojis, or small talk.\n",
        "- Be direct and medical in tone, like a triage nurse or clinical assistant.\n",
        "- Ask only one question at a time and wait for the user‚Äôs response.\n",
        "- Summarize and show the collected data only when the user types \"done\".\n",
        "- Format the summary in structured sections with labels: ‚ÄúSymptoms‚Äù, ‚ÄúOnset‚Äù, ‚ÄúTriggers‚Äù, ‚ÄúPain Description‚Äù, ‚ÄúAssociated Symptoms‚Äù, ‚ÄúMedical History‚Äù.\n",
        "\n",
        "Begin by asking:\n",
        "\"Hey I'm Hami! How are you feeling today?\"\n",
        "\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "print(\"ü©∫ AI Medical Assistant Ready (type 'done' to get the report)\\n\")\n",
        "\n",
        "# Send the initial message from the AI\n",
        "initial_response = chat.send_message(\"Hey I'm Hami! How are you feeling today?\")\n",
        "print(\"ü§ñ Assistant:\", initial_response.text)\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"üë§ You: \")\n",
        "    if user_input.lower() in [\"done\", \"exit\", \"quit\"]:\n",
        "        # Trigger the summary\n",
        "        response = chat.send_message(\"Please summarize the entire conversation in the format mentioned above.\")\n",
        "        print(\"\\nüìã Pre-Visit Summary:\\n\")\n",
        "        print(response.text)\n",
        "        break\n",
        "    else:\n",
        "        response = chat.send_message(user_input)\n",
        "        print(\"ü§ñ Assistant:\", response.text)"
      ],
      "metadata": {
        "id": "DcOvuV_vlru-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeFPKiVwlrs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xk9m-rbLlrq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vA4r_TySlrom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYK7Deujlrl2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}